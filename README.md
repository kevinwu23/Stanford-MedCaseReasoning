![Under Construction](https://img.shields.io/badge/status-under--construction-orange?logo=semantic-release&logoColor=white)
* There may be certain files missing as we are building out this repo in the next week. Plesae be patient with us!
  
# MedCaseReasoning

**An open-access benchmark and pipeline for evaluating and improving clinical diagnostic reasoning in large language models.**

[![HuggingFace](https://img.shields.io/badge/Dataset-HuggingFace-blue)](https://huggingface.co/datasets/zou-lab/MedCaseReasoning)
[![Paper](https://arxiv.org/abs/2505.11733)
[![License](https://img.shields.io/badge/License-CC--BY_4.0-green)](#license)

---

## âœ¨ Whatâ€™s inside?

| Split | # Cases | Purpose |
|-------|--------:|---------|
| **train** | 13,092 | Supervised fine-tuning & analysis |
| **test**  |   897 | Model-agnostic evaluation (diagnostic accuracy & reasoning recall) |

*Total*: **14,489 clinician-authored diagnostic cases** spanning 30+ medical specialties.

Each example contains

* `case_prompt` â€“ the patient presentation **before** a differential is made  
* `diagnostic_reasoning` â€“ numbered reasoning statements (with quotes from the article)  
* `final_diagnosis` â€“ single gold-standard diagnosis label  

Prompts are ~2.5Ã— longer than typical short-vignette datasets (e.g. MedQA, MMLU), mimicking real ward notes.

---

## ğŸ”§ Repository structure
<pre>
.
â”œâ”€â”€ prompts.py               # all prompt templates used in the paper
â”œâ”€â”€ download_pmc.py          # â‘  bulk PMC OA downloader
â”œâ”€â”€ get_case_report_pmcids.py# â‘¡ fetch case-report PMCIDs
â”œâ”€â”€ process_pmc.py           # â‘¢ parallel extractor for candidate XML
â”œâ”€â”€ extract_metadata.py      # â‘£ structured metadata
â”œâ”€â”€ extract_text.py          # â‘¤ clean XML â†’ text
â”œâ”€â”€ stitch_reasoning.py      # bullets â†’ fluent reasoning trace
â”œâ”€â”€ evaluate.py              # diagnostic accuracy & reasoning-recall runner
â””â”€â”€ finetune/
    â”œâ”€â”€ train_sft.py         # supervised fine-tuning recipe
    â””â”€â”€ configs/
</pre>

---

## ğŸš€ Quick start

### 1. Install

```bash
git clone https://github.com/kevinwu23/MedCaseReasoning.git
cd MedCaseReasoning
conda env create -f environment.yml
conda activate medcase
```

### 2. Load the dataset
```python
from datasets import load_dataset
ds = load_dataset("zou-lab/MedCaseReasoning", "all")  # or "train" / "test"
```
### 3. Evaluate an LLM
```bash
python evaluate.py \
    --model deepseek-ai/deepseek-llm-7b-chat \
    --shots 10 \
    --save results.json
```
evaluate.py reports:
	â€¢	Diagnostic accuracy (1/5/10-shot, LLM-as-judge)
	â€¢	Reasoning recall â€“ percentage of ground-truth reasoning points covered by the model trace

### 4. Fine-tune
```bash
python finetune/train_sft.py \
    --base_model "Qwen/Qwen2-7B-Instruct" \
    --train_file data/medcase_reasoning_train.jsonl \
    --eval_file  data/medcase_reasoning_test.jsonl \
    --config finetune/configs/sft_default.yaml
```

### ğŸ§‘â€ğŸ”¬ Reproducing the dataset

Run the pipeline end-to-end for full provenance:
#### 1.	Download bulk PMC XML
```bash
python download_pmc.py --year-from 2024
```

#### 2.	Identify case-report PMCIDs
```bash
python get_case_report_pmcids.py \
    --start-date 2015/01/01 \
    --email you@domain.com
```

#### 3.	Extract matching XML
```bash
python process_pmc.py
```

#### 4. Build JSONL dataset
```bash
python extract_metadata.py
python extract_text.py
python stitch_reasoning.py
```

### ğŸ“„ Citation

```bibtex
@inproceedings{wu2025medcase,
  title     = {MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports},
  author    = {Wu, Kevin and Wu, Eric and Thapa, Rahul and others},
  booktitle = {NeurIPS},
  year      = {2025},
  url       = {https://github.com/kevinwu23/MedCaseReasoning}
}
```

ğŸ”’ License
	â€¢	Code â€” MIT
	â€¢	Dataset â€” CC-BY 4.0 (derived from the PMC Open Access Subset)
	â€¢	Model checkpoints â€” see individual model cards

For questions, open an issue or email stanfordmedeval@gmail.com.
