# MedCaseReasoning

**An open-access benchmark and pipeline for evaluating and improving clinical diagnostic reasoning in large language models.**

[![HuggingFace](https://img.shields.io/badge/Dataset-HuggingFace-blue)](https://huggingface.co/datasets/zou-lab/MedCaseReasoning)
[![Paper](https://img.shields.io/badge/Paper-NeurIPS_2025-orange)](https://arxiv.org/abs/2505.11733) <!-- replace with final DOI/URL -->
[![License](https://img.shields.io/badge/License-CC-BY_4.0-green)](#license)

---

## ‚ú® What‚Äôs inside?

| Split | # Cases | Purpose |
|-------|--------:|---------|
| **train** | 13,092 | Supervised fine-tuning & analysis |
| **test**  |   897 | Model-agnostic evaluation (diagnostic accuracy & reasoning recall) |

*Total*: **14,489 clinician-authored diagnostic cases** spanning 30+ medical specialties.

Each example contains

* `case_prompt` ‚Äì the patient presentation **before** a differential is made  
* `diagnostic_reasoning` ‚Äì numbered reasoning statements (with quotes from the article)  
* `final_diagnosis` ‚Äì single gold-standard diagnosis label  

Prompts are ~2.5√ó longer than typical short-vignette datasets (e.g. MedQA, MMLU), mimicking real ward notes.

---

## üîß Repository structure
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ medcase_reasoning_train.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ medcase_reasoning_test.jsonl
‚îú‚îÄ‚îÄ prompts.py            # all prompt templates used in the paper
‚îú‚îÄ‚îÄ download_pmc.py       # ‚ë† bulk PMC OA downloader
‚îú‚îÄ‚îÄ get_case_report_pmcids.py   # ‚ë° fetch case-report PMCIDs
‚îú‚îÄ‚îÄ process_pmc.py        # ‚ë¢ parallel extractor for candidate XML
‚îú‚îÄ‚îÄ extract_metadata.py   # ‚ë£ structured metadata
‚îú‚îÄ‚îÄ extract_text.py       # ‚ë§ clean XML ‚Üí text
‚îú‚îÄ‚îÄ stitch_reasoning.py   # bullets ‚Üí fluent reasoning trace
‚îú‚îÄ‚îÄ evaluate.py           # diagnostic accuracy & reasoning-recall runner
‚îî‚îÄ‚îÄ finetune/
‚îú‚îÄ‚îÄ train_sft.py      # supervised fine-tuning recipe
‚îî‚îÄ‚îÄ configs/

> **Note**  
> The five numbered scripts reproduce the pipeline described in the paper (Fig 1 A) and yield the released dataset.  
> If you only need the ready-made data, skip to the HuggingFace section below.

---

## üöÄ Quick start

### 1. Install

```bash
git clone https://github.com/kevinwu23/MedCaseReasoning.git
cd MedCaseReasoning
conda env create -f environment.yml
conda activate medcase
```

### 2. Load the dataset
```python
from datasets import load_dataset
ds = load_dataset("zou-lab/MedCaseReasoning", "all")  # or "train" / "test"
```
### 3. Evaluate an LLM
```bash
python evaluate.py \
    --model deepseek-ai/deepseek-llm-7b-chat \
    --shots 10 \
    --save results.json
```
evaluate.py reports:
	‚Ä¢	Diagnostic accuracy (1/5/10-shot, LLM-as-judge)
	‚Ä¢	Reasoning recall ‚Äì percentage of ground-truth reasoning points covered by the model trace

### 4. Fine-tune
```bash
python finetune/train_sft.py \
    --base_model "Qwen/Qwen2-7B-Instruct" \
    --train_file data/medcase_reasoning_train.jsonl \
    --eval_file  data/medcase_reasoning_test.jsonl \
    --config finetune/configs/sft_default.yaml
```

### üßë‚Äçüî¨ Reproducing the dataset

Run the pipeline end-to-end for full provenance:
#### 1.	Download bulk PMC XML
```bash
python download_pmc.py --year-from 2024
```

#### 2.	Identify case-report PMCIDs
```bash
python get_case_report_pmcids.py \
    --start-date 2015/01/01 \
    --email you@domain.com
```

#### 3.	Extract matching XML
```bash
python process_pmc.py
```

#### 4. Build JSONL dataset
```bash
python extract_metadata.py
python extract_text.py
python stitch_reasoning.py
```

### üìÑ Citation

```bibtex
@inproceedings{wu2025medcase,
  title     = {MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports},
  author    = {Wu, Kevin and Wu, Eric and Thapa, Rahul and others},
  booktitle = {NeurIPS},
  year      = {2025},
  url       = {https://github.com/kevinwu23/MedCaseReasoning}
}
```

üîí License
	‚Ä¢	Code ‚Äî MIT
	‚Ä¢	Dataset ‚Äî CC-BY 4.0 (derived from the PMC Open Access Subset)
	‚Ä¢	Model checkpoints ‚Äî see individual model cards

For questions, open an issue or email stanfordmedeval@gmail.com.
